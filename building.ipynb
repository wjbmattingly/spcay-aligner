{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elizabeth': ['Liz', 'Lizzie', 'Beth', 'Betsy', 'Eliza'],\n",
       " 'William': ['Will', 'Bill', 'Billy', 'Liam'],\n",
       " 'Alexander': ['Alex', 'Xander', 'Zan', 'Sasha'],\n",
       " 'Margaret': ['Maggie', 'Meg', 'Peggy', 'Marge', 'Margie'],\n",
       " 'Katherine': ['Kate', 'Katie', 'Kathy', 'Kat'],\n",
       " 'Christopher': ['Chris', 'Topher', 'Kit', 'Christy'],\n",
       " 'Patricia': ['Pat', 'Patty', 'Tricia', 'Patsy'],\n",
       " 'Robert': ['Rob', 'Bob', 'Bobby', 'Bert'],\n",
       " 'Jennifer': ['Jen', 'Jenny', 'Jenna'],\n",
       " 'Michael': ['Mike', 'Mikey', 'Mick'],\n",
       " 'Sarah': ['Sally', 'Sadie', 'Sara', 'Sarita'],\n",
       " 'David': ['Dave', 'Davy', 'Dai', 'Davie'],\n",
       " 'Emily': ['Em', 'Emma', 'Emmie', 'Emmy'],\n",
       " 'Daniel': ['Dan', 'Danny', 'Dane', 'Danno'],\n",
       " 'Jessica': ['Jess', 'Jessie', 'Jessa', 'Jessy'],\n",
       " 'Matthew': ['Matt', 'Matty', 'Mat', 'Mats'],\n",
       " 'Amanda': ['Mandy', 'Manda', 'Mandi', 'Amy'],\n",
       " 'James': ['Jim', 'Jimmy', 'Jamie', 'Jem'],\n",
       " 'Laura': ['Laurie', 'Lora', 'Lolo', 'Lori'],\n",
       " 'John': ['Jon', 'Johnny', 'Jack', 'Jackie'],\n",
       " 'Rachel': ['Rae', 'Rach', 'Rachael', 'Rachie'],\n",
       " 'Thomas': ['Tom', 'Tommy', 'Thom', 'Tomas'],\n",
       " 'Rebecca': ['Becky', 'Becca', 'Bex', 'Reba'],\n",
       " 'Nicole': ['Nikki', 'Nicki', 'Cole', 'Nico'],\n",
       " 'Joshua': ['Josh', 'Joshie', 'Joshy', 'Joshua'],\n",
       " 'Lauren': ['Laur', 'Lolo', 'Lori', 'Lola'],\n",
       " 'Ryan': ['Ry', 'Rye', 'Ryry', 'Ryker'],\n",
       " 'Hannah': ['Han', 'Hanny', 'Hanna', 'Hannie'],\n",
       " 'Kevin': ['Kev', 'Kevvy', 'Vin', 'Kevie'],\n",
       " 'Stephanie': ['Steph', 'Steffi', 'Stevie', 'Stephie'],\n",
       " 'Andrew': ['Andy', 'Drew', 'Drewie', 'Andi'],\n",
       " 'Christina': ['Chris', 'Chrissy', 'Tina', 'Christy'],\n",
       " 'Justin': ['Just', 'Jus', 'Jay', 'Jussie'],\n",
       " 'Melissa': ['Mel', 'Melly', 'Missy', 'Lissa'],\n",
       " 'Brandon': ['Bran', 'Brand', 'Brando', 'Danno'],\n",
       " 'Kelly': ['Kel', 'Kell', 'Kellz', 'Kels'],\n",
       " 'Nathan': ['Nate', 'Natty', 'Nat', 'Nathanie'],\n",
       " 'Amy': ['Aims', 'Ames', 'Ami', 'Amie'],\n",
       " 'Kimberly': ['Kim', 'Kimmy', 'Kimmie', 'Kimi'],\n",
       " 'Timothy': ['Tim', 'Timmy', 'Timbo', 'Timmo'],\n",
       " 'Samantha': ['Sam', 'Sammie', 'Sammy', 'Sami'],\n",
       " 'Eric': ['Rick', 'Ricky', 'Rico', 'Erik'],\n",
       " 'Angela': ['Angie', 'Ange', 'Angel', 'Ang'],\n",
       " 'Gregory': ['Greg', 'Gregg', 'Greggie', 'Rory'],\n",
       " 'Heather': ['Heath', 'Heathy', 'Hettie', 'Hedy'],\n",
       " 'Benjamin': ['Ben', 'Benny', 'Benji', 'Benno']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"assets/links.json\", \"r\") as f:\n",
    "    links = json.load(f)\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Liz', 'Lizzie', 'Beth', 'Betsy', 'Eliza']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[\"Elizabeth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Elizabeth Jenkins went to school. Liz is 20. She also goes by Lizzie.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigError",
     "evalue": "unable to infer type for attribute \"links\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/wjbmattingly/projects/spcay-pmatch/building.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/spcay-pmatch/building.ipynb#W3sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         nx\u001b[39m.\u001b[39mdraw(G, pos, with_labels\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, node_color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlightblue\u001b[39m\u001b[39m'\u001b[39m, node_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, edge_color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m, linewidths\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, font_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/spcay-pmatch/building.ipynb#W3sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m         plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/spcay-pmatch/building.ipynb#W3sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m nlp\u001b[39m.\u001b[39;49madd_pipe(\u001b[39m\"\u001b[39;49m\u001b[39maligner\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/spcay-pmatch/building.ipynb#W3sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m doc \u001b[39m=\u001b[39m nlp(text)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/spacy/language.py:821\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    817\u001b[0m     pipe_component, factory_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_pipe_from_source(\n\u001b[1;32m    818\u001b[0m         factory_name, source, name\u001b[39m=\u001b[39mname\n\u001b[1;32m    819\u001b[0m     )\n\u001b[1;32m    820\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 821\u001b[0m     pipe_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_pipe(\n\u001b[1;32m    822\u001b[0m         factory_name,\n\u001b[1;32m    823\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    824\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    825\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[1;32m    826\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    827\u001b[0m     )\n\u001b[1;32m    828\u001b[0m pipe_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[1;32m    829\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_meta[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/spacy/language.py:709\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    706\u001b[0m cfg \u001b[39m=\u001b[39m {factory_name: config}\n\u001b[1;32m    707\u001b[0m \u001b[39m# We're calling the internal _fill here to avoid constructing the\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39m# registered functions twice\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m resolved \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39;49mresolve(cfg, validate\u001b[39m=\u001b[39;49mvalidate)\n\u001b[1;32m    710\u001b[0m filled \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39mfill({\u001b[39m\"\u001b[39m\u001b[39mcfg\u001b[39m\u001b[39m\"\u001b[39m: cfg[factory_name]}, validate\u001b[39m=\u001b[39mvalidate)[\u001b[39m\"\u001b[39m\u001b[39mcfg\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    711\u001b[0m filled \u001b[39m=\u001b[39m Config(filled)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/confection/__init__.py:756\u001b[0m, in \u001b[0;36mregistry.resolve\u001b[0;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    748\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresolve\u001b[39m(\n\u001b[1;32m    749\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    754\u001b[0m     validate: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    755\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 756\u001b[0m     resolved, _ \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_make(\n\u001b[1;32m    757\u001b[0m         config, schema\u001b[39m=\u001b[39;49mschema, overrides\u001b[39m=\u001b[39;49moverrides, validate\u001b[39m=\u001b[39;49mvalidate, resolve\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    758\u001b[0m     )\n\u001b[1;32m    759\u001b[0m     \u001b[39mreturn\u001b[39;00m resolved\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/confection/__init__.py:805\u001b[0m, in \u001b[0;36mregistry._make\u001b[0;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_interpolated:\n\u001b[1;32m    804\u001b[0m     config \u001b[39m=\u001b[39m Config(orig_config)\u001b[39m.\u001b[39minterpolate()\n\u001b[0;32m--> 805\u001b[0m filled, _, resolved \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_fill(\n\u001b[1;32m    806\u001b[0m     config, schema, validate\u001b[39m=\u001b[39;49mvalidate, overrides\u001b[39m=\u001b[39;49moverrides, resolve\u001b[39m=\u001b[39;49mresolve\n\u001b[1;32m    807\u001b[0m )\n\u001b[1;32m    808\u001b[0m filled \u001b[39m=\u001b[39m Config(filled, section_order\u001b[39m=\u001b[39msection_order)\n\u001b[1;32m    809\u001b[0m \u001b[39m# Check that overrides didn't include invalid properties not in config\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/confection/__init__.py:859\u001b[0m, in \u001b[0;36mregistry._fill\u001b[0;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[1;32m    857\u001b[0m     field \u001b[39m=\u001b[39m schema\u001b[39m.\u001b[39m__fields__[key]\n\u001b[1;32m    858\u001b[0m     schema\u001b[39m.\u001b[39m__fields__[key] \u001b[39m=\u001b[39m copy_model_field(field, Any)\n\u001b[0;32m--> 859\u001b[0m promise_schema \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mmake_promise_schema(value, resolve\u001b[39m=\u001b[39;49mresolve)\n\u001b[1;32m    860\u001b[0m filled[key], validation[v_key], final[key] \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_fill(\n\u001b[1;32m    861\u001b[0m     value,\n\u001b[1;32m    862\u001b[0m     promise_schema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    866\u001b[0m     overrides\u001b[39m=\u001b[39moverrides,\n\u001b[1;32m    867\u001b[0m )\n\u001b[1;32m    868\u001b[0m reg_name, func_name \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_constructor(final[key])\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/confection/__init__.py:1068\u001b[0m, in \u001b[0;36mregistry.make_promise_schema\u001b[0;34m(cls, obj, resolve)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         sig_args[name] \u001b[39m=\u001b[39m (annotation, default)\n\u001b[1;32m   1067\u001b[0m sig_args[\u001b[39m\"\u001b[39m\u001b[39m__config__\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _PromiseSchemaConfig\n\u001b[0;32m-> 1068\u001b[0m \u001b[39mreturn\u001b[39;00m create_model(\u001b[39m\"\u001b[39;49m\u001b[39mArgModel\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msig_args)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/pydantic/main.py:1024\u001b[0m, in \u001b[0;36mpydantic.main.create_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/pydantic/main.py:221\u001b[0m, in \u001b[0;36mpydantic.main.ModelMetaclass.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/pydantic/fields.py:504\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/pydantic/fields.py:434\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/pydantic/fields.py:544\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.prepare\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/holocaust/lib/python3.10/site-packages/pydantic/fields.py:576\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField._set_default_and_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mConfigError\u001b[0m: unable to infer type for attribute \"links\""
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import networkx\n",
    "from collections import defaultdict\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"connections\", default=defaultdict(list))\n",
    "Doc.set_extension(\"graph\", default=nx.Graph())\n",
    "Doc.set_extension(\"links\", default={})\n",
    "\n",
    "links = {\"Elizabeth\": [\"Liz\", \"Lizzie\", \"Beth\", \"Betsy\", \"Eliza\"],}\n",
    "\n",
    "text = \"Elizabeth Jenkins went to school. Liz is 20. She also goes by Lizzie. She is Mrs. Jenkins to her students. Once she completes her PhD, she will be Dr. Elizabeth P. Jenkins.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "@Language.factory(\"aligner\",\n",
    "                  assigns=[\"doc._.connections\", \"doc._.graph\"])\n",
    "class Aligner:\n",
    "    def __init__(self,\n",
    "                 nlp: Language,\n",
    "                 name: str,\n",
    "                 links: {}\n",
    "                 ):\n",
    "        \n",
    "        self.nlp = nlp\n",
    "        self.links = links\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        connect_parts(doc)\n",
    "        connect_nicknames(doc)\n",
    "        \n",
    "\n",
    "    def _connect_parts(self, doc):\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                for token in ent:\n",
    "                    self.doc._.connections[token.text].append(ent)\n",
    "        return doc\n",
    "    \n",
    "    def connect_nicknames(doc, links):\n",
    "        connections = defaultdict(list)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                main_connected = False\n",
    "                for token in ent:\n",
    "                    for main_name, variants in links.items():\n",
    "                        # Connect every token that matches a main name or its variants directly to the main name\n",
    "                        if token.text == main_name or token.text in variants:\n",
    "                            if not main_connected:  # Only add the main name once per entity\n",
    "                                connections[main_name].append(ent.text)\n",
    "                                main_connected = True\n",
    "                            # Link each variant to the main name\n",
    "                            connections[token.text].append(main_name)\n",
    "        return connections\n",
    "    \n",
    "    def build_graph(self, doc):\n",
    "        for token, entities in self.connections.items():\n",
    "            self.doc._.graph.add_node(token)\n",
    "            for entity in entities:\n",
    "                if entity.text != token:  # Avoid self-loops\n",
    "                    self.doc._.graph.add_edge(token, entity.text)\n",
    "        return G\n",
    "\n",
    "    def query_graph(self,doc, node, depth=1):\n",
    "        if node not in self.doc._.graph:\n",
    "            print(f\"Node '{node}' not found in the graph.\")\n",
    "            return None\n",
    "\n",
    "        # Use BFS to get all connections up to 'depth' levels deep\n",
    "        subgraph_nodes = set()\n",
    "        for d in range(depth + 1):  # Include the depth level itself\n",
    "            for neighbor in nx.single_source_shortest_path_length(G, node, cutoff=d):\n",
    "                subgraph_nodes.add(neighbor)\n",
    "\n",
    "        # Create a subgraph with the nodes found\n",
    "        subgraph = G.subgraph(subgraph_nodes)\n",
    "        visualize_graph(subgraph)\n",
    "\n",
    "    def visualize_graph(G):\n",
    "        pos = nx.spring_layout(G)  # Generate positions for all nodes\n",
    "        nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=100, edge_color='gray', linewidths=1, font_size=10)\n",
    "        plt.show()\n",
    "\n",
    "nlp.add_pipe(\"aligner\")\n",
    "doc = nlp(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'Elizabeth': ['Elizabeth Jenkins', 'Elizabeth', 'Liz', 'Lizzie', 'Elizabeth P. Jenkins', 'Elizabeth'], 'Liz': ['Elizabeth'], 'Lizzie': ['Elizabeth']})\n",
      "Graph with 7 nodes and 5 edges\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def connect_parts(doc):\n",
    "    connections = defaultdict(list)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            for token in ent:\n",
    "                connections[token.text].append(ent)\n",
    "    return connections\n",
    "\n",
    "def build_graph(connections):\n",
    "    G = nx.Graph()\n",
    "    # Add nodes and edges based on connections in the dictionary\n",
    "    for token, entities in connections.items():\n",
    "        G.add_node(token)\n",
    "        for entity in entities:\n",
    "            # Ensure every entity text that includes this token links back to the token\n",
    "            if entity.text != token:  # Avoid self-loops\n",
    "                G.add_edge(token, entity.text)\n",
    "    print(G)\n",
    "    return G\n",
    "\n",
    "def query_graph(G, node, depth=1):\n",
    "    if node not in G:\n",
    "        print(f\"Node '{node}' not found in the graph.\")\n",
    "        return None\n",
    "\n",
    "    # Use BFS to get all connections up to 'depth' levels deep\n",
    "    subgraph_nodes = set()\n",
    "    for d in range(depth + 1):  # Include the depth level itself\n",
    "        for neighbor in nx.single_source_shortest_path_length(G, node, cutoff=d):\n",
    "            subgraph_nodes.add(neighbor)\n",
    "\n",
    "    # Create a subgraph with the nodes found\n",
    "    subgraph = G.subgraph(subgraph_nodes)\n",
    "    visualize_graph(subgraph)\n",
    "\n",
    "def visualize_graph(G):\n",
    "    pos = nx.spring_layout(G)  # Generate positions for all nodes\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=100, edge_color='gray', linewidths=1, font_size=10)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def connect_nicknames(doc, links):\n",
    "    connections = defaultdict(list)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            main_connected = False\n",
    "            for token in ent:\n",
    "                for main_name, variants in links.items():\n",
    "                    # Connect every token that matches a main name or its variants directly to the main name\n",
    "                    if token.text == main_name or token.text in variants:\n",
    "                        if not main_connected:  # Only add the main name once per entity\n",
    "                            connections[main_name].append(ent.text)\n",
    "                            main_connected = True\n",
    "                        # Link each variant to the main name\n",
    "                        connections[token.text].append(main_name)\n",
    "    return connections\n",
    "\n",
    "connections = connect_parts(doc)\n",
    "new_links = connect_nicknames(doc, links)\n",
    "G = build_graph(connections)\n",
    "query_graph(G, \"Elizabeth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holocaust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
